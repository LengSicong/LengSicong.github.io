---
permalink: /
title: "About Me"
excerpt: "Welcome to my page!"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

## Introduction
Hello! I'm Sicong Leng, a 2nd-year P.h.D. at Nanyang Technological University. I am currently under Alibaba-NTU talent programme and jointly supervised by [Prof.Lu Shijian](https://personal.ntu.edu.sg/shijian.lu/) ([Visual-Intelligence Lab](https://sg-vilab.github.io/)/[S-lab](https://www.ntu.edu.sg/s-lab)) and [Dr.Bing Lidong](https://lidongbing.github.io/) ([Alibaba DAMO Academy](https://github.com/DAMO-NLP-SG)).

I specialize in Deep Learning with a focus on Multi-modality research, especially for Vision+Language.
Feel free to reach out to me for collaborations, questions, or just to chat!

## News
* [24.10] CMM has been released! Check out our project [here](https://cmm-damovl.site).
* [24.09] 1 paper accepted by **NeurIPS 2024**! Congratulations to the co-authors!
* [24.06] VideoLLaMA 2 has been released! Check out our paper and code [here](https://github.com/DAMO-NLP-SG/VideoLLaMA2)
* [24.04] VCD has received the **CVPR 2024 Highlight**!
* [24.03] 3 papers accepted by **CVPR 2024**! Congratulations to the co-authors!
* [23.11] VCD has been released! Check out our paper and code [here](https://github.com/DAMO-NLP-SG/VCD)
* [23.08] We present our work at Nvidia Internal Technical Sharing!
* [23.08] We present our work at AAAI 2023 Summer Symposium Series!
* [23.07] Tell2Design has received the **Area Chair Award** and **Best Paper Nomination** at **ACL 2023**!
* [23.06] Our paper [Tell2Design](https://arxiv.org/abs/2311.15941) has been accepted by **ACL 2023** as a long oral paper!
* [21.06] Our paper about [Video Grounding](https://arxiv.org/abs/2106.11013) has been accepted by **CVPR 2021** as a long paper!
  
## Awards
* CVPR 2024 Highlight
* ACL 2023 Area Chair Award
* ACL 2023 Best Paper Nomination

<!-- red color is used for highlighting  -->
## Selected Publications
* Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding [[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Leng_Mitigating_Object_Hallucinations_in_Large_Vision-Language_Models_through_Visual_Contrastive_CVPR_2024_paper.pdf) [[code]](https://github.com/DAMO-NLP-SG/VCD)
  * **Sicong Leng**\*, Hang Zhang\*, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, Lidong Bing
  * CVPR 2024 $\color{red}{\text{(Highlight)}}$

* VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs [[paper]](https://arxiv.org/abs/2406.07476) [[code]](https://github.com/DAMO-NLP-SG/VideoLLaMA2)
  * Zesen Cheng\*, **Sicong Leng**\*, Hang Zhang\*, Yifei Xin\*, Xin Li\*, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, Lidong Bing
  * ArXiv 2024

* The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio [[paper]]() [[project]](cmm-damovl.site) [[code]](https://github.com/DAMO-NLP-SG/CMM)
  * **Sicong Leng**\*, Yun Xing\*, Zesen Cheng\*, Yang Zhou, Hang Zhang, Xin Li, Deli Zhao, Shijian Lu, Chunyan Miao, Lidong Bing 
  * ArXiv 2024

* Tell2Design: A Dataset for Language-Guided Floor Plan Generation [[paper]](https://arxiv.org/abs/2311.15941) [[code]](https://github.com/LengSicong/Tell2Design)
  * **Sicong Leng**\*, Yang Zhou\*, Mohammed Haroon Dupty, Wee Sun Lee, Sam Conrad Joyce, Wei Lu
  * ACL 2023 $\color{red}{\text{(Area Chair Award) (Best Paper Nomination)}}$

Please refer to [Google Scholar](https://scholar.google.com/citations?user=xQsBP6YAAAAJ&hl=en) for the full list of publications.

## Service 
* Reviewer:
  * 2025: NAACL
  * 2024: EMNLP, WACV
  * 2023: EMNLP, CoNLL, NIPS, ACL
* Program Committee:
  * EMNLP 2023 Industry Track

## Work experience
* Aug 2021 - Aug 2023: Research Assistant
  * [StatNLP Lab](https://statnlp-research.github.io/), Singapore University of Technology and Design
  * Research on NLP and Multi-modal Learning
  * Supervisor: Professor [Lu Wei](https://istd.sutd.edu.sg/people/faculty/lu-wei/)


Website last updated on 16th October 2024.

<!-- ## News
  <ul>{% for post in site.talks %}
    {% include archive-single-talk.html %}
  {% endfor %}</ul>

## Publications
  <ul>{% for post in site.publications %}
    {% include archive-single.html %}
  {% endfor %}</ul> -->
